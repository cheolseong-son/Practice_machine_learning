{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 개요\n",
    "+ 파이썬으로 구현된 쉽고 간결한 딥러닝 라이브러리\n",
    "+ 내부적으로는 텐서플로우 엔진이 구동되지만 직관적인 API로 쉽게 딥러닝 실험을 할 수 있도록 지원\n",
    "\n",
    "\n",
    "#### 2. 주요 특징\n",
    "+ 모듈화\n",
    "+ 최소주의\n",
    "+ 쉬운 확장성\n",
    "+ 파이썬 기반\n",
    "\n",
    "\n",
    "#### 3. Document\n",
    "+ https://keras.io/\n",
    "\n",
    "\n",
    "#### 4. 주요 객체\n",
    "+ Sequential()\n",
    "    + loss\n",
    "        - 다중분류 : 'categorical_crossentropy\n",
    "        - 이진분류 : 'binary_crossentropy\n",
    "        - 선형회귀 : 'mean_squared_error\n",
    "    + optimizer\n",
    "        - SGD (확률적 경사하강법)\n",
    "        - Adam\n",
    "        - Adamax\n",
    "        - Nadam\n",
    "        - RMSprop\n",
    "        - Adabelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import matplotlib.font_manager as fm\n",
    "# fond_name = fm.FontProperties(fname=\"C:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "# plt.rc('font',family=font_name)\n",
    "\n",
    "# mpl.rcParams['axes.unicode_minus']=False\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8b414c84fe60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Weight\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"bias\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m_variable_v2_call\u001b[1;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maggregation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m       \u001b[0maggregation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariableAggregation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m     return previous_getter(\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[0minitial_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(**kws)\u001b[0m\n\u001b[0;32m    235\u001b[0m                         shape=None):\n\u001b[0;32m    236\u001b[0m     \u001b[1;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m     \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdefault_variable_creator_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator_v2\u001b[1;34m(next_creator, **kwargs)\u001b[0m\n\u001b[0;32m   2631\u001b[0m   \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2633\u001b[1;33m   return resource_variable_ops.ResourceVariable(\n\u001b[0m\u001b[0;32m   2634\u001b[0m       \u001b[0minitial_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m       \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m   1505\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_from_proto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimport_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1507\u001b[1;33m       self._init_from_args(\n\u001b[0m\u001b[0;32m   1508\u001b[0m           \u001b[0minitial_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[1;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[0;32m   1649\u001b[0m           \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Initializer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1650\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[1;32m-> 1651\u001b[1;33m                 \u001b[0minitial_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1652\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[0;32m   1653\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'shape'"
     ]
    }
   ],
   "source": [
    "x_train = [1,2,3]\n",
    "y_train = [1,2,3]\n",
    "\n",
    "W = tf.Variable(tf.random_normal_initializer([1]), dtype=tf.float32, name=\"Weight\")\n",
    "b = tf.Variable(tf.random_normal_initializer([1]), dtype=tf.float32, name=\"bias\")\n",
    "\n",
    "# 가설 준비\n",
    "hypot = W*x_train + b\n",
    "\n",
    "# 비용 함수(평균제곱 오차함수)\n",
    "cost = tf.reduce_mean(tf.square(hypot - y_train))\n",
    "\n",
    "# 최저 비용 학습을 위한 경사 하강 알고리즘 (Gradient Dscent)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimizer(cost)\n",
    "\n",
    "#================ Graph 작업 완료===============\n",
    "sess = tf.Session()\n",
    "sess = run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1001):\n",
    "    _, weight, c, bias = sess.run([train, W, cost, bias],\n",
    "                          feed_dict = {x_train:[1,2,3,4,5],\n",
    "                                       y_train:[2.1,3.1,4.1,5.1,6.1]})\n",
    "    if step % 100 == 0:\n",
    "        print(step, weight, c, bias)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 4.9751\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.7888\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.6062\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.4274\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 4.2523\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 4.0812\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.9140\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.7507\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.5915\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4363\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.2851\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.1381\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.9952\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.8564\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7217\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.5911\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4647\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.3424\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2242\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1101\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0001\n",
      "Epoch 22/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.8940\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.7920\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.6939\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.5997\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.5093\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.4226\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.3397\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.2605\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1848\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - 0s 997us/step - loss: 1.1126\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.0439\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9785\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.9163\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8573\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.8014\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.7485\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6984\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6512\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6067\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5647\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5253\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4883\n",
      "Epoch 44/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.4537\n",
      "Epoch 45/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.421 - 0s 2ms/step - loss: 0.4212\n",
      "Epoch 46/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3909\n",
      "Epoch 47/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3626\n",
      "Epoch 48/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.3362\n",
      "Epoch 49/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3117\n",
      "Epoch 50/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2889\n",
      "Epoch 51/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2678\n",
      "Epoch 52/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2483\n",
      "Epoch 53/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2302\n",
      "Epoch 54/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2136\n",
      "Epoch 55/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1982\n",
      "Epoch 56/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1841\n",
      "Epoch 57/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1712\n",
      "Epoch 58/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1594\n",
      "Epoch 59/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1486\n",
      "Epoch 60/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1387\n",
      "Epoch 61/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1298\n",
      "Epoch 62/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1216\n",
      "Epoch 63/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1143\n",
      "Epoch 64/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.1076\n",
      "Epoch 65/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1016\n",
      "Epoch 66/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0962\n",
      "Epoch 67/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0913\n",
      "Epoch 68/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0869\n",
      "Epoch 69/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0830\n",
      "Epoch 70/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0796\n",
      "Epoch 71/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0765\n",
      "Epoch 72/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0737\n",
      "Epoch 73/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0713\n",
      "Epoch 74/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0692\n",
      "Epoch 75/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0673\n",
      "Epoch 76/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0656\n",
      "Epoch 77/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0642\n",
      "Epoch 78/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0629\n",
      "Epoch 79/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0618\n",
      "Epoch 80/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0608\n",
      "Epoch 81/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0600\n",
      "Epoch 82/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0592\n",
      "Epoch 83/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0586\n",
      "Epoch 84/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0581\n",
      "Epoch 85/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0576\n",
      "Epoch 86/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0572\n",
      "Epoch 87/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0568\n",
      "Epoch 88/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0565\n",
      "Epoch 89/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0562\n",
      "Epoch 90/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0560\n",
      "Epoch 91/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0557\n",
      "Epoch 92/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0555\n",
      "Epoch 93/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0553\n",
      "Epoch 94/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0552\n",
      "Epoch 95/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0550\n",
      "Epoch 96/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0548\n",
      "Epoch 97/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0547\n",
      "Epoch 98/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0545\n",
      "Epoch 99/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0544\n",
      "Epoch 100/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0543\n",
      "Epoch 101/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0541\n",
      "Epoch 102/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0540\n",
      "Epoch 103/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0538\n",
      "Epoch 104/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0537\n",
      "Epoch 105/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0535\n",
      "Epoch 106/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0533\n",
      "Epoch 107/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0532\n",
      "Epoch 108/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0530\n",
      "Epoch 109/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0529\n",
      "Epoch 110/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0527\n",
      "Epoch 111/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0525\n",
      "Epoch 112/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0524\n",
      "Epoch 113/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0522\n",
      "Epoch 114/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0520\n",
      "Epoch 115/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0518\n",
      "Epoch 116/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0517\n",
      "Epoch 117/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0515\n",
      "Epoch 118/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0513\n",
      "Epoch 119/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0511\n",
      "Epoch 120/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0510\n",
      "Epoch 121/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0508\n",
      "Epoch 122/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0506\n",
      "Epoch 123/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0504\n",
      "Epoch 124/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0502\n",
      "Epoch 125/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0501\n",
      "Epoch 126/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0499\n",
      "Epoch 127/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0497\n",
      "Epoch 128/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0495\n",
      "Epoch 129/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0493\n",
      "Epoch 130/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0492\n",
      "Epoch 131/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0490\n",
      "Epoch 132/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0488\n",
      "Epoch 133/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0486\n",
      "Epoch 134/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0484\n",
      "Epoch 135/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0483\n",
      "Epoch 136/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0481\n",
      "Epoch 137/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0479\n",
      "Epoch 138/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0477\n",
      "Epoch 139/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0476\n",
      "Epoch 140/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0474\n",
      "Epoch 141/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0472\n",
      "Epoch 142/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0470\n",
      "Epoch 143/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0469\n",
      "Epoch 144/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0467\n",
      "Epoch 145/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0465\n",
      "Epoch 146/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0463\n",
      "Epoch 147/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0462\n",
      "Epoch 148/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0460\n",
      "Epoch 149/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0458\n",
      "Epoch 150/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0457\n",
      "Epoch 151/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0455\n",
      "Epoch 152/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0453\n",
      "Epoch 153/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0451\n",
      "Epoch 154/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0450\n",
      "Epoch 155/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0448\n",
      "Epoch 156/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0446\n",
      "Epoch 157/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0444\n",
      "Epoch 158/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0443\n",
      "Epoch 159/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0441\n",
      "Epoch 160/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0439\n",
      "Epoch 161/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0438\n",
      "Epoch 162/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0436\n",
      "Epoch 163/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0434\n",
      "Epoch 164/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0432\n",
      "Epoch 165/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0431\n",
      "Epoch 166/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0429\n",
      "Epoch 167/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0427\n",
      "Epoch 168/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0426\n",
      "Epoch 169/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0424\n",
      "Epoch 170/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0422\n",
      "Epoch 171/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0420\n",
      "Epoch 172/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0419\n",
      "Epoch 173/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0417\n",
      "Epoch 174/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0415\n",
      "Epoch 175/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0414\n",
      "Epoch 176/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0412\n",
      "Epoch 177/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0410\n",
      "Epoch 178/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0408\n",
      "Epoch 179/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0407\n",
      "Epoch 180/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0405\n",
      "Epoch 181/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0403\n",
      "Epoch 182/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0402\n",
      "Epoch 183/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0400\n",
      "Epoch 184/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0398\n",
      "Epoch 185/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0397\n",
      "Epoch 186/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0395\n",
      "Epoch 187/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0393\n",
      "Epoch 188/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0392\n",
      "Epoch 189/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0390\n",
      "Epoch 190/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0388\n",
      "Epoch 191/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0386\n",
      "Epoch 192/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0385\n",
      "Epoch 193/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0383\n",
      "Epoch 194/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0381\n",
      "Epoch 195/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0380\n",
      "Epoch 196/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0378\n",
      "Epoch 197/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0376\n",
      "Epoch 198/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0375\n",
      "Epoch 199/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0373\n",
      "Epoch 200/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0372\n",
      "Epoch 201/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0370\n",
      "Epoch 202/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0368\n",
      "Epoch 203/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0367\n",
      "Epoch 204/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0365\n",
      "Epoch 205/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0363\n",
      "Epoch 206/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0362\n",
      "Epoch 207/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0360\n",
      "Epoch 208/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0358\n",
      "Epoch 209/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0357\n",
      "Epoch 210/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0355\n",
      "Epoch 211/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0354\n",
      "Epoch 212/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0352\n",
      "Epoch 213/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0350\n",
      "Epoch 214/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0349\n",
      "Epoch 215/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0347\n",
      "Epoch 216/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0345\n",
      "Epoch 217/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0344\n",
      "Epoch 218/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0342\n",
      "Epoch 219/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0341\n",
      "Epoch 220/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 221/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0338\n",
      "Epoch 222/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0336\n",
      "Epoch 223/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0334\n",
      "Epoch 224/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0333\n",
      "Epoch 225/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0331\n",
      "Epoch 226/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0330\n",
      "Epoch 227/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0328\n",
      "Epoch 228/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0326\n",
      "Epoch 229/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0325\n",
      "Epoch 230/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0323\n",
      "Epoch 231/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0322\n",
      "Epoch 232/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0320\n",
      "Epoch 233/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0319\n",
      "Epoch 234/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0317\n",
      "Epoch 235/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0316\n",
      "Epoch 236/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 237/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0313\n",
      "Epoch 238/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0311\n",
      "Epoch 239/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0310\n",
      "Epoch 240/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0308\n",
      "Epoch 241/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0307\n",
      "Epoch 242/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0305\n",
      "Epoch 243/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0303\n",
      "Epoch 244/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0302\n",
      "Epoch 245/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0300\n",
      "Epoch 246/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0299\n",
      "Epoch 247/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0297\n",
      "Epoch 248/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0296\n",
      "Epoch 249/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0295\n",
      "Epoch 250/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0293\n",
      "Epoch 251/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0292\n",
      "Epoch 252/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0290\n",
      "Epoch 253/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0289\n",
      "Epoch 254/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0287\n",
      "Epoch 255/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0286\n",
      "Epoch 256/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0284\n",
      "Epoch 257/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0283\n",
      "Epoch 258/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0281\n",
      "Epoch 259/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0280\n",
      "Epoch 260/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0278\n",
      "Epoch 261/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0277\n",
      "Epoch 262/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0276\n",
      "Epoch 263/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0274\n",
      "Epoch 264/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0273\n",
      "Epoch 265/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0271\n",
      "Epoch 266/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0270\n",
      "Epoch 267/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0269\n",
      "Epoch 268/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0267\n",
      "Epoch 269/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0266\n",
      "Epoch 270/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0264\n",
      "Epoch 271/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0263\n",
      "Epoch 272/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0262\n",
      "Epoch 273/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0260\n",
      "Epoch 274/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0259\n",
      "Epoch 275/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0257\n",
      "Epoch 276/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0256\n",
      "Epoch 277/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0255\n",
      "Epoch 278/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0253\n",
      "Epoch 279/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0252\n",
      "Epoch 280/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0251\n",
      "Epoch 281/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0249\n",
      "Epoch 282/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0248\n",
      "Epoch 283/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0247\n",
      "Epoch 284/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0245\n",
      "Epoch 285/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0244\n",
      "Epoch 286/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0243\n",
      "Epoch 287/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0241\n",
      "Epoch 288/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0240\n",
      "Epoch 289/400\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.0239\n",
      "Epoch 290/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0237\n",
      "Epoch 291/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0236\n",
      "Epoch 292/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0235\n",
      "Epoch 293/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0233\n",
      "Epoch 294/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0232\n",
      "Epoch 295/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0231\n",
      "Epoch 296/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0230\n",
      "Epoch 297/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0228\n",
      "Epoch 298/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0227\n",
      "Epoch 299/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0226\n",
      "Epoch 300/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0225\n",
      "Epoch 301/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0223\n",
      "Epoch 302/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0222\n",
      "Epoch 303/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0221\n",
      "Epoch 304/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0220\n",
      "Epoch 305/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0218\n",
      "Epoch 306/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0217\n",
      "Epoch 307/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0216\n",
      "Epoch 308/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0215\n",
      "Epoch 309/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0213\n",
      "Epoch 310/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0212\n",
      "Epoch 311/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0211\n",
      "Epoch 312/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0210\n",
      "Epoch 313/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0209\n",
      "Epoch 314/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0207\n",
      "Epoch 315/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0206\n",
      "Epoch 316/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0205\n",
      "Epoch 317/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204\n",
      "Epoch 318/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203\n",
      "Epoch 319/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0201\n",
      "Epoch 320/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200\n",
      "Epoch 321/400\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.0199\n",
      "Epoch 322/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198\n",
      "Epoch 323/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197\n",
      "Epoch 324/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196\n",
      "Epoch 325/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0194\n",
      "Epoch 326/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193\n",
      "Epoch 327/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192\n",
      "Epoch 328/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191\n",
      "Epoch 329/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190\n",
      "Epoch 330/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189\n",
      "Epoch 331/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0188\n",
      "Epoch 332/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187\n",
      "Epoch 333/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 334/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0184\n",
      "Epoch 335/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0183\n",
      "Epoch 336/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0182\n",
      "Epoch 337/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0181\n",
      "Epoch 338/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0180\n",
      "Epoch 339/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0179\n",
      "Epoch 340/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0178\n",
      "Epoch 341/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.017 - 0s 2ms/step - loss: 0.0177\n",
      "Epoch 342/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0176\n",
      "Epoch 343/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0175\n",
      "Epoch 344/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0174\n",
      "Epoch 345/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0173\n",
      "Epoch 346/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0171\n",
      "Epoch 347/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0170\n",
      "Epoch 348/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0169\n",
      "Epoch 349/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0168\n",
      "Epoch 350/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0167\n",
      "Epoch 351/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0166\n",
      "Epoch 352/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0165\n",
      "Epoch 353/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0164\n",
      "Epoch 354/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0163\n",
      "Epoch 355/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0162\n",
      "Epoch 356/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0161\n",
      "Epoch 357/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0160\n",
      "Epoch 358/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0159\n",
      "Epoch 359/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0158\n",
      "Epoch 360/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0157\n",
      "Epoch 361/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0156\n",
      "Epoch 362/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0155\n",
      "Epoch 363/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0154\n",
      "Epoch 364/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0153\n",
      "Epoch 365/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0152\n",
      "Epoch 366/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0151\n",
      "Epoch 367/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0150\n",
      "Epoch 368/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0150\n",
      "Epoch 369/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0149\n",
      "Epoch 370/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0148\n",
      "Epoch 371/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0147\n",
      "Epoch 372/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0146\n",
      "Epoch 373/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0145\n",
      "Epoch 374/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0144\n",
      "Epoch 375/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0143\n",
      "Epoch 376/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0142\n",
      "Epoch 377/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0141\n",
      "Epoch 378/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0140\n",
      "Epoch 379/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0139\n",
      "Epoch 380/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0138\n",
      "Epoch 381/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0138\n",
      "Epoch 382/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0137\n",
      "Epoch 383/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0136\n",
      "Epoch 384/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0135\n",
      "Epoch 385/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 386/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0133\n",
      "Epoch 387/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0132\n",
      "Epoch 388/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0131\n",
      "Epoch 389/400\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0131\n",
      "Epoch 390/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0130\n",
      "Epoch 391/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0129\n",
      "Epoch 392/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0128\n",
      "Epoch 393/400\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.012 - 0s 2ms/step - loss: 0.0127\n",
      "Epoch 394/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0126\n",
      "Epoch 395/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0125\n",
      "Epoch 396/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0125\n",
      "Epoch 397/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0124\n",
      "Epoch 398/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0123\n",
      "Epoch 399/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0122\n",
      "Epoch 400/400\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0121\n",
      "dict_keys(['loss'])\n",
      "[4.975085258483887, 4.788790225982666, 4.60619592666626, 4.427361965179443, 4.252347469329834, 4.081206321716309, 3.913985013961792, 3.750730514526367, 3.5914814472198486, 3.4362735748291016, 3.285135269165039, 3.138091564178467, 2.995159864425659, 2.856353998184204, 2.7216808795928955, 2.5911407470703125, 2.4647295475006104, 2.342437267303467, 2.2242484092712402, 2.110140085220337, 2.00008487701416, 1.894049882888794, 1.7919965982437134, 1.6938807964324951, 1.5996544361114502, 1.5092636346817017, 1.4226486682891846, 1.339747428894043, 1.2604925632476807, 1.1848126649856567, 1.1126327514648438, 1.0438746213912964, 0.9784566164016724, 0.9162936210632324, 0.8572991490364075, 0.8013836741447449, 0.7484559416770935, 0.6984225511550903, 0.6511895060539246, 0.606661319732666, 0.5647412538528442, 0.5253326892852783, 0.4883386194705963, 0.45366206765174866, 0.4212065637111664, 0.3908756673336029, 0.36257439851760864, 0.3362084925174713, 0.3116845488548279, 0.28891173005104065, 0.26779964566230774, 0.2482607364654541, 0.23020882904529572, 0.21356001496315002, 0.1982332468032837, 0.18414905667304993, 0.17123083770275116, 0.1594051718711853, 0.14860039949417114, 0.13874807953834534, 0.12978269159793854, 0.12164105474948883, 0.11426340043544769, 0.10759258270263672, 0.10157406330108643, 0.09615641832351685, 0.09129081666469574, 0.08693128824234009, 0.08303456008434296, 0.07955984026193619, 0.07646912336349487, 0.0737268254160881, 0.07129968702793121, 0.06915688514709473, 0.06726980209350586, 0.06561194360256195, 0.06415905803442001, 0.06288860738277435, 0.06177999824285507, 0.06081442907452583, 0.059974830597639084, 0.05924554914236069, 0.05861254781484604, 0.058063067495822906, 0.057585738599300385, 0.057170115411281586, 0.05680730193853378, 0.056489042937755585, 0.05620810389518738, 0.055958282202482224, 0.05573398992419243, 0.055530376732349396, 0.05534332990646362, 0.05516914278268814, 0.05500484257936478, 0.05484774708747864, 0.054695792496204376, 0.054547011852264404, 0.05440002679824829, 0.05425361543893814, 0.054106831550598145, 0.053958993405103683, 0.053809456527233124, 0.053657881915569305, 0.05350397899746895, 0.053347665816545486, 0.053188811987638474, 0.053027499467134476, 0.052863817662000656, 0.0526977963745594, 0.05252961069345474, 0.05235954374074936, 0.05218764394521713, 0.05201416462659836, 0.051839299499988556, 0.05166326090693474, 0.05148615688085556, 0.05130833387374878, 0.051129840314388275, 0.05095086246728897, 0.05077158287167549, 0.05059205740690231, 0.05041239410638809, 0.05023273825645447, 0.0500531904399395, 0.04987379536032677, 0.04969460889697075, 0.04951559007167816, 0.04933694005012512, 0.04915863275527954, 0.048980630934238434, 0.04880301654338837, 0.04862568527460098, 0.04844873398542404, 0.04827221482992172, 0.048095934092998505, 0.047919973731040955, 0.04774439334869385, 0.04756907373666763, 0.04739397019147873, 0.0472191721200943, 0.04704459011554718, 0.046870213001966476, 0.04669605568051338, 0.04652209207415581, 0.04634823277592659, 0.04617452621459961, 0.046000976115465164, 0.04582761973142624, 0.0456542894244194, 0.04548110440373421, 0.04530801251530647, 0.045135051012039185, 0.04496217146515846, 0.04478931427001953, 0.04461657255887985, 0.044443875551223755, 0.044271353632211685, 0.04409889131784439, 0.043926484882831573, 0.04375416785478592, 0.04358193650841713, 0.043409787118434906, 0.043237753212451935, 0.043065838515758514, 0.04289402440190315, 0.04272225871682167, 0.04255067557096481, 0.04237920418381691, 0.042207855731248856, 0.04203661158680916, 0.0418655090034008, 0.04169461876153946, 0.04152383282780647, 0.04135320335626602, 0.0411827452480793, 0.041012443602085114, 0.04084233194589615, 0.04067233204841614, 0.04050262272357941, 0.040333040058612823, 0.04016367718577385, 0.0399944968521595, 0.03982548043131828, 0.03965669497847557, 0.03948815166950226, 0.039319783449172974, 0.039151646196842194, 0.03898371011018753, 0.03881604224443436, 0.03864855691790581, 0.03848135471343994, 0.038314394652843475, 0.03814762085676193, 0.037981171160936356, 0.03781493753194809, 0.037648994475603104, 0.03748326748609543, 0.037317849695682526, 0.03715262934565544, 0.0369877927005291, 0.036823105067014694, 0.03665872663259506, 0.036494702100753784, 0.036330897361040115, 0.03616740554571152, 0.036004215478897095, 0.03584126755595207, 0.03567863255739212, 0.035516366362571716, 0.035354338586330414, 0.03519265726208687, 0.03503124788403511, 0.03487018123269081, 0.034709446132183075, 0.03454900532960892, 0.03438887745141983, 0.03422912210226059, 0.034069597721099854, 0.033910512924194336, 0.0337517112493515, 0.03359328582882881, 0.03343517333269119, 0.03327737748622894, 0.033119939267635345, 0.03296291083097458, 0.03280622512102127, 0.03264982998371124, 0.03249381110072136, 0.03233816474676132, 0.032182853668928146, 0.03202797472476959, 0.0318734310567379, 0.03171924501657486, 0.031565409153699875, 0.03141196817159653, 0.031258903443813324, 0.03110622428357601, 0.030953938141465187, 0.030802030116319656, 0.03065047785639763, 0.030499327927827835, 0.030348574742674828, 0.0301982369273901, 0.030048225075006485, 0.029898658394813538, 0.02974950149655342, 0.02960074506700039, 0.02945232391357422, 0.02930435538291931, 0.029156770557165146, 0.0290096215903759, 0.02886286936700344, 0.02871651016175747, 0.028570573776960373, 0.02842503786087036, 0.02827993594110012, 0.02813524566590786, 0.027990981936454773, 0.02784712053835392, 0.027703696861863136, 0.027560699731111526, 0.027418097481131554, 0.027275925502181053, 0.027134204283356667, 0.026992926374077797, 0.026852035894989967, 0.026711583137512207, 0.02657158300280571, 0.026431981474161148, 0.0262928307056427, 0.026154130697250366, 0.026015862822532654, 0.025878001004457474, 0.02574062906205654, 0.02560368739068508, 0.025467127561569214, 0.02533108927309513, 0.025195419788360596, 0.02506026066839695, 0.024925529956817627, 0.02479122020304203, 0.024657344445586205, 0.024523967877030373, 0.02439102903008461, 0.02425852045416832, 0.024126438423991203, 0.02399485558271408, 0.023863647133111954, 0.02373300865292549, 0.02360272780060768, 0.02347293123602867, 0.023343611508607864, 0.02321472391486168, 0.02308628335595131, 0.022958312183618546, 0.022830763831734657, 0.02270372211933136, 0.02257712185382843, 0.022450964897871017, 0.022325286641716957, 0.022200031206011772, 0.022075282409787178, 0.021950922906398773, 0.021827057003974915, 0.02170366421341896, 0.021580742672085762, 0.02145826816558838, 0.021336250007152557, 0.021214690059423447, 0.021093588322401047, 0.020972952246665955, 0.020852791145443916, 0.020733052864670753, 0.02061384543776512, 0.020495038479566574, 0.02037673257291317, 0.02025885134935379, 0.02014145627617836, 0.02002452127635479, 0.019908074289560318, 0.019792020320892334, 0.01967649534344673, 0.019561365246772766, 0.019446764141321182, 0.019332578405737877, 0.019218873232603073, 0.019105613231658936, 0.01899280957877636, 0.018880527466535568, 0.018768688663840294, 0.018657268956303596, 0.018546339124441147, 0.018435854464769363, 0.01832585409283638, 0.018216285854578018, 0.018107183277606964, 0.017998531460762024, 0.01789035275578499, 0.01778266206383705, 0.01767539046704769, 0.017568595707416534, 0.01746222749352455, 0.01735634170472622, 0.0172509104013443, 0.017145931720733643, 0.01704142615199089, 0.016937363892793655, 0.016833757981657982, 0.016730615869164467, 0.01662791147828102, 0.016525672748684883, 0.01642388291656971, 0.01632256619632244, 0.01622168719768524, 0.016121260821819305, 0.016021253541111946, 0.01592174358665943, 0.015822656452655792, 0.01572403311729431, 0.01562586985528469, 0.015528124757111073, 0.015430813655257225, 0.015334007330238819, 0.015237584710121155, 0.01514164824038744, 0.015046164393424988, 0.014951096847653389, 0.01485646702349186, 0.01476230751723051, 0.014668578281998634, 0.014575295150279999, 0.014482458122074604, 0.01439001876860857, 0.01429806835949421, 0.014206510968506336, 0.014115418307483196, 0.014024746604263783, 0.013934527523815632, 0.01384477037936449, 0.01375538855791092, 0.013666471466422081, 0.01357799582183361, 0.01348993182182312, 0.013402308337390423, 0.013315098360180855, 0.013228344731032848, 0.01314198412001133, 0.01305608730763197, 0.012970572337508202, 0.01288550067692995, 0.012800885364413261, 0.012716652825474739, 0.012632873840630054, 0.012549476698040962, 0.012466518208384514, 0.012383995577692986, 0.01230187714099884, 0.012220179662108421, 0.012138905003666878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.2817388]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 위 예제를 keras로 표현\n",
    "\n",
    "x_train = [1,2,3,4,5]\n",
    "y_train = [2.1, 3.1, 4.1,5.1,6.1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(tf.keras.layers.Dense(1, input_shape=(1,))) # 1은 출력의 개수\n",
    "# input_shape=(1,) :1은 입력의 개수\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=\"mse\")\n",
    "\n",
    "# 학습하고 결과값 리턴\n",
    "hist = model.fit(x_train, y_train, epochs=400)\n",
    "\n",
    "# 가중치와 바이어스도 출력\n",
    "model.get_weights()\n",
    "\n",
    "print(hist.history.keys())\n",
    "print(hist.history[\"loss\"])\n",
    "\n",
    "plt.plot(hist.history['loss']) # 그래프로 loss값 확인\n",
    "\n",
    "# 예측값 확인\n",
    "print(model.predict([6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1,2,3,4,5,6,7]\n",
    "y = [ 25000, 55000, 75000, 110000, 128000, 155000, 180000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(tf.keras.layers.Dense(1, input_shape=(1,)))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss=\"mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습하고 결과값 리턴\n",
    "hist = model.fit(X, y, epochs=30)\n",
    "plt.plot(hist.history['loss']) # 그래프로 loss값 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값 확인\n",
    "print(model.predict([8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 선형회귀 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제(체중과 나이에 따른 혈당 분석)\n",
    "# 체중이 100, 나이 40일때 혈당은?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 5)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "data = np.genfromtxt('../Acorn machine learning/x09.txt',skip_header=36)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(data[: , 2:4], dtype=np.float32)\n",
    "y = np.array(data[: , 4], dtype=np.float32)\n",
    "print(X.shape)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-8089ea93cdd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(tf.keras.layers.Dense(2, input_shape=(2,)))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습하고 결과값 리턴\n",
    "hist = model.fit(X, y, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실값 그래프로 확인\n",
    "plt.plot(hist.history['loss']) # 그래프로 loss값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict(np.array([[100,40]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주택 가격을 사용한 회귀 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_target), (test_data, test_target) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape, test_data.shape)\n",
    "print(train_target[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2개의 레이어로 구성 : 입출력 개수는 64개\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(64, input_shape=(13,), activation=\"relu\")) # 출력 개수 64개\n",
    "model.add(Dense(64, activation=\"relu\")) # 출력개수 64개, 입력개수는 두번째 레이어부터는 생략\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=\"rmsprop\", metrics=[\"mae\"])\n",
    "# metrics=[\"mae\"] :평균절대값 오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit(train_data, train_target, epochs=100, batch_size=1)\n",
    "# batch_size : 기본값이 1이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss']) # 그래프로 loss값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과 확인\n",
    "mse, mae = model.evaluate(test_data, test_target)\n",
    "print(mse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data-= mean\n",
    "\n",
    "std = train_data.std(axis=0)\n",
    "train_dd/=std\n",
    "\n",
    "test_data = -= mean\n",
    "test_data/=std\n",
    "# 정규화 후 다시 학습시키면 차이가 있다. 정규화 하는 것이 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로지스틱  회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2], [2,3], [3,1],[4,3], [5, 3], [6,2] ]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "\n",
    "X = tf.placeholder(tf.float, shape=[None,2])\n",
    "y = tf.placeholder(tf.float, shape=[None,1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([2,1]))\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "# 가설\n",
    "hypot = tf.sigmoid(tf.matmul(X, W) + b )\n",
    "\n",
    "# 비용 함수\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis + (1 - Y) * tf.log(1 - hypothesis)))\n",
    "\n",
    "# 최소비용\n",
    "train = tf.train.GridientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 모델을 Keras 알고리즘으로 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[1,2], [2,3], [3,1],[4,3], [5, 3], [6,2]])\n",
    "y_data = np.array([[0], [0], [0], [1], [1], [1]])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=2, activation=\"relu\")) \n",
    "# input_dim=2 ==input_shape(2,)\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.RMSprop(lr=0.1), metrics=[\"binary_accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "model.fit(x_data, y_data, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost, acc = model.evaluate(x_data, y_data)\n",
    "print(cost,acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 책 6,7장 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 첫번째 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터 읽어 들이기\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 float32 자료형으로 변환하고 정규화하기\n",
    "\n",
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')\n",
    "X_train/=255\n",
    "X_test/=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 데이터를 0-9가지의 카테고리를 나타내는 배열로 변환하기\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "# print(y_train.shape)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조 정의하기\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구축하기\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 훈련\n",
    "\n",
    "hist = model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터로 평가하기\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('loss =', score[0])\n",
    "print('accuracy = ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras로 비만도 판정해보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMI 데이터를 읽어 들이고 정규화 하기\n",
    "\n",
    "csv = pd.read_csv(\"../Acorn machine learning/bmi.csv\")\n",
    "# 몸무게와 키 데이터\n",
    "csv['weight'] /= 100\n",
    "csv['height'] /= 200  \n",
    "\n",
    "X = csv[['weight', 'height']].values\n",
    "\n",
    "bclass = {'thin': [1, 0, 0], 'normal': [0, 1, 0], 'fat': [0, 0, 1]}\n",
    "\n",
    "y = np.empty((20000, 3))\n",
    "for i, v in enumerate(csv['label']):\n",
    "    y[i] = bclass[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 전용 데이터와 테스트 전용 데이터로 나누기\n",
    "\n",
    "X_train, y_train = X[1:15001], y[1:15001]\n",
    "X_test, y_test = X[15001:20001], y[15001:20001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구조 정의하기\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(2,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구축하기\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 훈련하기\n",
    "\n",
    "hist = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=100,\n",
    "    epochs=20,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=2)], # 데이터 오류시 중지\n",
    "    verbose=1\n",
    ")\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터로 평가하기\n",
    "\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('loss=', score[0])\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 두번째"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# ont-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train) # 숫자를 원핫인코딩으로 바꿈\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 표준화\n",
    "X_train = X_train.reshape(-1,28*28).astype(\"float32\") /255\n",
    "X_test = X_test.reshape(-1,28*28).astype(\"float32\") /255\n",
    "\n",
    "# validation 분활(k-fold) : 12600개를 테스트용 나머지는 \n",
    "# 훈련용으로 돌아가면서 학습\n",
    "x_val = X_train[:12600]\n",
    "X_train = X_train[12600:]\n",
    "\n",
    "y_val = y_train[:12600]\n",
    "y_train = y_train[12600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(Dense(64, input_shape=(28*28,), activation=\"relu\"))\n",
    "model.add(Dense(units=64, input_dim=28*28, activation=\"relu\"))\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001),\n",
    "             metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# 학습 \n",
    "model.fit(X_train, y_train, epochs=15, batch_size=32,\n",
    "         validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가\n",
    "score = model.evaluate(X_test, y_test, verbose=1, batch_size=32)\n",
    "print('loss =', score[0])\n",
    "print('accuracy = ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 세번째 형태: 텐서보드, 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# ont-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 표준화\n",
    "X_train = X_train.reshape(-1,28*28).astype(\"float32\") /255\n",
    "X_test = X_test.reshape(-1,28*28).astype(\"float32\") /255\n",
    "\n",
    "# validation 분활(k-fold) : 12600개를 테스트용 나머지는 \n",
    "# 훈련용으로 돌아가면서 학습\n",
    "x_val = X_train[:12600]\n",
    "X_train = X_train[12600:]\n",
    "\n",
    "y_val = y_train[:12600]\n",
    "y_train = y_train[12600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# model.add(Dense(64, input_shape=(28*28,), activation=\"relu\"))\n",
    "model.add(Dense(units=64, input_dim=28*28, activation=\"relu\"))\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001),\n",
    "             metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# 텐서 보드 설정 : tensorboard--logdir = graph\n",
    "tf_hist = keras.callbacks.TensorBoard(log_dir=\"../graph\", write_graph=True, \n",
    "                           write_images=True)\n",
    "\n",
    "\n",
    "# 학습 \n",
    "model.fit(X_train, y_train, epochs=15, batch_size=32,\n",
    "         validation_data=(x_val, y_val), callbacks=[tf_hist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련한 결과가 필요할 때 불러 올 수 있다.\n",
    "# 모델 저장\n",
    "model.save(\"../Acorn machine learning/mnist_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장 모델 불러오기\n",
    "\n",
    "model2 = load_model(\"../Acorn machine learning/mnist_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가\n",
    "score = model2.evaluate(X_test, y_test, verbose=1, batch_size=32)\n",
    "print('loss =', score[0])\n",
    "print('accuracy = ', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 조기 종료(EarlyStopping)\n",
    "\n",
    "EarlyStopping(monitor=\"val_loss\", min_data=0, patience=10) # 기본값\n",
    "\n",
    "+ monitor : 관찰하고자 하는 항목, 주로 val_loss나 val_acc가 사용됨\n",
    "+ min_data : 개선되고 있다고 판단하기 위한 최소 변화량. 보통 0을 지정\n",
    "+ patience : 지정된 값까지 기다렸다가 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# ont-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 표준화\n",
    "X_train = X_train.reshape(-1,28*28).astype(\"float32\") /255\n",
    "X_test = X_test.reshape(-1,28*28).astype(\"float32\") /255\n",
    "\n",
    "# validation 분활(k-fold) : 12600개를 테스트용 나머지는 \n",
    "# 훈련용으로 돌아가면서 학습\n",
    "x_val = X_train[:12600]\n",
    "X_train = X_train[12600:]\n",
    "\n",
    "y_val = y_train[:12600]\n",
    "y_train = y_train[12600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# model.add(Dense(64, input_shape=(28*28,), activation=\"relu\"))\n",
    "model.add(Dense(units=64, input_dim=28*28, activation=\"relu\"))\n",
    "model.add(Dense(units=64, activation=\"relu\"))\n",
    "model.add(Dense(units=10, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001),\n",
    "             metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# 텐서 보드 설정 : tensorboard--logdir = graph\n",
    "tf_hist = keras.callbacks.TensorBoard(log_dir=\"../graph\", write_graph=True, \n",
    "                           write_images=True)\n",
    "\n",
    "early = EarlyStopping() # 기본값\n",
    "\n",
    "# 학습 \n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32,\n",
    "         validation_data=(x_val, y_val), callbacks=[tf_hist,early])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 함수화, dropout\n",
    "+ dropout은 과적합을 막기 위한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_num, output_num, hidden_layer):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(hidden_layer[0], input_shape=(input_num,),\n",
    "                    activation=\"relu\", name=\"hidden-1\"))\n",
    "    model.add(Dense(hidden_layer[1], activation=\"relu\", name=\"hidden-2\")) \n",
    "    model.add(Dense(hidden_layer[2], activation=\"relu\", name=\"hidden-3\"))\n",
    "    model.add(Dense(hidden_layer[3], activation=\"relu\", name=\"hidden-4\"))\n",
    "    \n",
    "    # dropout\n",
    "    model.add(keras.layers.core.Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(output_num, activation=\"softmax\"))\n",
    "    \n",
    "    # 모델 컴파일\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# ont-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 표준화\n",
    "X_train = X_train.reshape(-1,28*28).astype(\"float32\") /255\n",
    "X_test = X_test.reshape(-1,28*28).astype(\"float32\") /255\n",
    "\n",
    "# 파라미터\n",
    "input_num = 784\n",
    "output_num = 10\n",
    "hidden_layer = [255,255,255,255] # \n",
    "\n",
    "model = make_model(input_num, output_num, hidden_layer)\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "perform = model.evaluate(X_test, y_test, batch_size=100)\n",
    "print(perform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 붓꽃 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\")\n",
    "iris\n",
    "\n",
    "X = iris.iloc[:, :4].values\n",
    "y = iris.iloc[:, -1].values\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자를 숫자로 바꾸고 원핫인코딩으로 만들기\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y1 = encoder.fit_transform(y)\n",
    "y1\n",
    "y2 = pd.get_dummies(y1).values\n",
    "y2 # one-hot encoding\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.2,\n",
    "                                                   random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 및 학습\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(4,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
    "             metrics=[\"accuracy\"])\n",
    "\n",
    "hist = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "\n",
    "print(hist.history.keys())\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.plot(hist.history[\"val_loss\"])\n",
    "plt.plot(hist.history[\"accuracy\"])\n",
    "plt.plot(hist.history[\"val_accuracy\"])\n",
    "\n",
    "plt.legend([\"loss\", \"val_loss\", \"accuracy\", \"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 테스트\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "print(confusion_matrix(y_test_class, y_pred_class))\n",
    "\n",
    "print('__________________________________________')\n",
    "test_set = np.array([[5,2.9, 1, 0.2]])\n",
    "print(\"품종 예측 :\",iris[\"species\"].unique()[model.predict_classes(test_set)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST 를 이용한 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution Layer 모델 작성\n",
    "# 필터 개수 : 3개, 필터 크기 : 3*3\n",
    "from keras import layers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 첫번째 conv layer\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu', # 32개 출력\n",
    "                              input_shape=(28,28,1))) \n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "# --------------------- first layer -------------------------\n",
    "\n",
    "# 두번째 conv layer\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu')) # 64개 출력\n",
    "model.add(keras.layers.MaxPool2D(2,2)) \n",
    "\n",
    "# 세번재 conv layer\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu')) # 64개 출력\n",
    "\n",
    "# FC\n",
    "model.add(layers.Flatten())\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\")) # 최정 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# ont-hot encoding\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 표준화\n",
    "X_train = X_train.reshape(60000,28,28,1).astype(\"float32\") /255\n",
    "X_test = X_test.reshape(10000,28,28,1).astype(\"float32\") /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
    "             metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "model.fit(X_train, y_train,epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN-LSTM\n",
    "\n",
    "##### 1. 샘플 분석\n",
    "\n",
    "    + 로이터 뉴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train news article : 8982\n",
      "test news article : 2246\n",
      "category : 46\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분류 \n",
    "# num_words = : 데이터에서 등장 빈도 순위로 몇 번째 해당하는 단어까지 사용할 것이지 지정\n",
    "# None이라고 하면 모든 단어 사용\n",
    "\n",
    "(X_train, y_train),(X_test, y_test) = reuters.load_data(num_words=None, \n",
    "                                                        test_split=0.2) # 이 데이터 옵션\n",
    "\n",
    "print(\"train news article :\", len(X_train))\n",
    "print(\"test news article :\",len(X_test))\n",
    "print(\"category :\", max(y_train)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first training news article\n",
    "#  # 단어를 숫자로 변환된것이며 빈도수 등수별로 변환됨\n",
    "# 1이 빈도수 가장 많음\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 뉴스 기사의 길이\n",
    "print(\"최대 길이 :\", max(len(l) for l in X_train))\n",
    "print(\"평균 길이 :\", sum(map(len, X_train)) / len(X_train))\n",
    "\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.ylabel(\"count of samples\")\n",
    "plt.xlabel(\"length of samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  카테고리별 빈도수\n",
    "fig, axe = plt.subplots(ncols=1)\n",
    "fig.set_size_inches(12,5)\n",
    "sns.countplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리별 정확한 빈도수 확인\n",
    "unique_element, counts_element = np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어별 인데스(빈도수에 대한 순위) 확인\n",
    "word_to_index = reuters.get_word_index()\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 단어만 보고 싶을 때\n",
    "index_to_word = {}\n",
    "for key, value in word_to_index.items():\n",
    "    index_to_word[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"빈도수 상위 28842번째 단어 : \",index_to_word[28842])\n",
    "print(\"빈도수 상위 1번째 단어 : \", index_to_word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코딩\n",
    "print(\"\".join([index_to_word[index] for index in X_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(X_train, y_train),(X_test, y_test) = reuters.load_data(num_words=30000, \n",
    "                                                        test_split=0.3,\n",
    "                                                       maxlen=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 길이 : 49\n",
      "[   1  245  273  207  156   53   74  160   26   14   46  296   26   39\n",
      "   74 2979 3554   14   46 4689 4329   86   61 3499 4795   14   61  451\n",
      " 4329   17   12    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# 모든 문장의 길이를 동일한 길이로 맍춰준다.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train = pad_sequences(X_train, padding=\"post\")\n",
    "X_test = pad_sequences(X_test, padding=\"post\")\n",
    "print(\"문장 길이 :\",len(X_train[0]))\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1395, 49, 1)\n",
      "(599, 49, 1)\n"
     ]
    }
   ],
   "source": [
    "# 3차원으로 shape을 맞춰준다.\n",
    "\n",
    "X_train = X_train.reshape((-1, 49,1)) \n",
    "# -1 or 1395, X_train.shape[0] 라고 해도 된다.\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1)) # 이런 형식도 OK\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599,) (1395,)\n",
      "(1994,)\n",
      "(1994, 46)\n",
      "(1395, 46)\n",
      "(599, 46)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "print(y_test.shape, y_train.shape) \n",
    "y_data = np.concatenate((y_train, y_test)) # 연결\n",
    "print(y_data.shape)\n",
    "\n",
    "y_data = to_categorical(y_data)\n",
    "print(y_data.shape)\n",
    "y_data\n",
    "\n",
    "y_train = y_data[:1395]\n",
    "y_test = y_data[1395:]\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  3. Vanilla RNN을 이용한 다중 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단층 RNN\n",
    "# return_sequences\n",
    "#     메모리셀이 모든 시점(Time Step)에 대해서 은닉상태값을 출력할 때 True\n",
    "#     다층 모델일 경우에도 True\n",
    "#     메모리셀이 하나으 은닉 상택값만 출력할 때 False\n",
    "\n",
    "def vanilla_rnn():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(50, input_shape=(49,1), return_sequences=False)) \n",
    "    # 출력 : 50개, 훈련데이터의 개수 :(49,1)\n",
    "    model.add(Dense(46, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=Adam(lr=0.001), \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 2.7762 - accuracy: 0.4222\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.3378 - accuracy: 0.7133\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.2084 - accuracy: 0.7147\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 1.1778 - accuracy: 0.7147\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1688 - accuracy: 0.7147\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1650 - accuracy: 0.7147\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1603 - accuracy: 0.7147\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1576 - accuracy: 0.7147\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 1.1539 - accuracy: 0.7147\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 1.1528 - accuracy: 0.7147\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1529 - accuracy: 0.7147\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1504 - accuracy: 0.7147\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1501 - accuracy: 0.7147\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1481 - accuracy: 0.7147\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1472 - accuracy: 0.7147\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1554 - accuracy: 0.7147\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1480 - accuracy: 0.7147\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1437 - accuracy: 0.7147\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1453 - accuracy: 0.7147\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1447 - accuracy: 0.7147\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1443 - accuracy: 0.7147\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1397 - accuracy: 0.7147\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1385 - accuracy: 0.7147\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1338 - accuracy: 0.7147\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1228 - accuracy: 0.7147\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1289 - accuracy: 0.7147\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1406 - accuracy: 0.7168\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1302 - accuracy: 0.7176\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1200 - accuracy: 0.7133\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0977 - accuracy: 0.7154\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1294 - accuracy: 0.7133\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1347 - accuracy: 0.7147\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.1142 - accuracy: 0.7211\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0869 - accuracy: 0.7154\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0828 - accuracy: 0.7183\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0743 - accuracy: 0.7133\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0855 - accuracy: 0.7219\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0737 - accuracy: 0.7140\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0599 - accuracy: 0.7197\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0338 - accuracy: 0.7176\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0331 - accuracy: 0.7190\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0239 - accuracy: 0.7168\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0250 - accuracy: 0.7269\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0153 - accuracy: 0.7140\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0197 - accuracy: 0.7204\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0155 - accuracy: 0.7197\n",
      "Epoch 47/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0216 - accuracy: 0.7197\n",
      "Epoch 48/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0044 - accuracy: 0.7197\n",
      "Epoch 49/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0080 - accuracy: 0.7262\n",
      "Epoch 50/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0242 - accuracy: 0.7197\n",
      "Epoch 51/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9968 - accuracy: 0.7233\n",
      "Epoch 52/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0029 - accuracy: 0.7211\n",
      "Epoch 53/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9924 - accuracy: 0.7233\n",
      "Epoch 54/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9924 - accuracy: 0.7176\n",
      "Epoch 55/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.9888 - accuracy: 0.7297\n",
      "Epoch 56/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.9982 - accuracy: 0.7290\n",
      "Epoch 57/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9961 - accuracy: 0.7219\n",
      "Epoch 58/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9954 - accuracy: 0.7276\n",
      "Epoch 59/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0010 - accuracy: 0.7176\n",
      "Epoch 60/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9841 - accuracy: 0.7233\n",
      "Epoch 61/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9834 - accuracy: 0.7297\n",
      "Epoch 62/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9788 - accuracy: 0.7233\n",
      "Epoch 63/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0824 - accuracy: 0.7254\n",
      "Epoch 64/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9990 - accuracy: 0.7211\n",
      "Epoch 65/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9804 - accuracy: 0.7262\n",
      "Epoch 66/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9716 - accuracy: 0.7283\n",
      "Epoch 67/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9836 - accuracy: 0.7211\n",
      "Epoch 68/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9638 - accuracy: 0.7297\n",
      "Epoch 69/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9692 - accuracy: 0.7262\n",
      "Epoch 70/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.9788 - accuracy: 0.7297\n",
      "Epoch 71/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9615 - accuracy: 0.7305\n",
      "Epoch 72/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.9716 - accuracy: 0.7376\n",
      "Epoch 73/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9797 - accuracy: 0.7247\n",
      "Epoch 74/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9565 - accuracy: 0.7333\n",
      "Epoch 75/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9518 - accuracy: 0.7384\n",
      "Epoch 76/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9481 - accuracy: 0.7362\n",
      "Epoch 77/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9508 - accuracy: 0.7305\n",
      "Epoch 78/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9592 - accuracy: 0.7297\n",
      "Epoch 79/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9486 - accuracy: 0.7297\n",
      "Epoch 80/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9415 - accuracy: 0.7326\n",
      "Epoch 81/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9400 - accuracy: 0.7362\n",
      "Epoch 82/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9484 - accuracy: 0.7391\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 8ms/step - loss: 0.9609 - accuracy: 0.7297\n",
      "Epoch 84/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9314 - accuracy: 0.7348\n",
      "Epoch 85/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9207 - accuracy: 0.7348\n",
      "Epoch 86/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9264 - accuracy: 0.7290\n",
      "Epoch 87/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9850 - accuracy: 0.7254\n",
      "Epoch 88/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0010 - accuracy: 0.7254\n",
      "Epoch 89/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9670 - accuracy: 0.7219\n",
      "Epoch 90/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9466 - accuracy: 0.7283\n",
      "Epoch 91/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9324 - accuracy: 0.7362\n",
      "Epoch 92/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9295 - accuracy: 0.7326\n",
      "Epoch 93/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9191 - accuracy: 0.7348\n",
      "Epoch 94/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9074 - accuracy: 0.7376\n",
      "Epoch 95/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8986 - accuracy: 0.7384\n",
      "Epoch 96/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9023 - accuracy: 0.7376\n",
      "Epoch 97/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9034 - accuracy: 0.7305\n",
      "Epoch 98/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8981 - accuracy: 0.7333\n",
      "Epoch 99/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0024 - accuracy: 0.7233\n",
      "Epoch 100/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 1.0393 - accuracy: 0.7190\n",
      "Epoch 101/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9830 - accuracy: 0.7283\n",
      "Epoch 102/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9383 - accuracy: 0.7319\n",
      "Epoch 103/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.9208 - accuracy: 0.7326\n",
      "Epoch 104/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.9109 - accuracy: 0.7355\n",
      "Epoch 105/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8924 - accuracy: 0.7412\n",
      "Epoch 106/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8879 - accuracy: 0.7376\n",
      "Epoch 107/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8901 - accuracy: 0.7355\n",
      "Epoch 108/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8806 - accuracy: 0.7419\n",
      "Epoch 109/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8801 - accuracy: 0.7405\n",
      "Epoch 110/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8765 - accuracy: 0.7355\n",
      "Epoch 111/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8690 - accuracy: 0.7362\n",
      "Epoch 112/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8821 - accuracy: 0.7398\n",
      "Epoch 113/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8788 - accuracy: 0.7384\n",
      "Epoch 114/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8645 - accuracy: 0.7405\n",
      "Epoch 115/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8636 - accuracy: 0.7405\n",
      "Epoch 116/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8702 - accuracy: 0.7405\n",
      "Epoch 117/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8647 - accuracy: 0.7391\n",
      "Epoch 118/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8592 - accuracy: 0.7419\n",
      "Epoch 119/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8709 - accuracy: 0.7376\n",
      "Epoch 120/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8701 - accuracy: 0.7427\n",
      "Epoch 121/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8571 - accuracy: 0.7434\n",
      "Epoch 122/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8565 - accuracy: 0.7412\n",
      "Epoch 123/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8501 - accuracy: 0.7355\n",
      "Epoch 124/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8614 - accuracy: 0.7455\n",
      "Epoch 125/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8699 - accuracy: 0.7362\n",
      "Epoch 126/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8540 - accuracy: 0.7384\n",
      "Epoch 127/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8786 - accuracy: 0.7333\n",
      "Epoch 128/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8669 - accuracy: 0.7398\n",
      "Epoch 129/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8642 - accuracy: 0.7405\n",
      "Epoch 130/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8704 - accuracy: 0.7419\n",
      "Epoch 131/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8533 - accuracy: 0.7427\n",
      "Epoch 132/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8447 - accuracy: 0.7434\n",
      "Epoch 133/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8454 - accuracy: 0.7391\n",
      "Epoch 134/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8457 - accuracy: 0.7441\n",
      "Epoch 135/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8439 - accuracy: 0.7498\n",
      "Epoch 136/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8377 - accuracy: 0.7427\n",
      "Epoch 137/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8379 - accuracy: 0.7462\n",
      "Epoch 138/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8480 - accuracy: 0.7362\n",
      "Epoch 139/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8472 - accuracy: 0.7434\n",
      "Epoch 140/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8370 - accuracy: 0.7448\n",
      "Epoch 141/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.8766 - accuracy: 0.7434\n",
      "Epoch 142/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8699 - accuracy: 0.7391\n",
      "Epoch 143/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8452 - accuracy: 0.7441\n",
      "Epoch 144/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8336 - accuracy: 0.7484\n",
      "Epoch 145/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8381 - accuracy: 0.7491\n",
      "Epoch 146/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8364 - accuracy: 0.7520\n",
      "Epoch 147/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8419 - accuracy: 0.7405\n",
      "Epoch 148/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8333 - accuracy: 0.7462\n",
      "Epoch 149/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.8270 - accuracy: 0.7527\n",
      "Epoch 150/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.8206 - accuracy: 0.7520\n",
      "Epoch 151/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8272 - accuracy: 0.7484\n",
      "Epoch 152/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8267 - accuracy: 0.7484\n",
      "Epoch 153/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8232 - accuracy: 0.7477\n",
      "Epoch 154/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8176 - accuracy: 0.7498\n",
      "Epoch 155/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8544 - accuracy: 0.7470\n",
      "Epoch 156/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8379 - accuracy: 0.7455\n",
      "Epoch 157/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8228 - accuracy: 0.7448\n",
      "Epoch 158/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8143 - accuracy: 0.7520\n",
      "Epoch 159/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8179 - accuracy: 0.7541\n",
      "Epoch 160/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8246 - accuracy: 0.7441\n",
      "Epoch 161/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8191 - accuracy: 0.7520\n",
      "Epoch 162/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8140 - accuracy: 0.7513\n",
      "Epoch 163/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8096 - accuracy: 0.7470\n",
      "Epoch 164/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8077 - accuracy: 0.7534\n",
      "Epoch 165/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.8114 - accuracy: 0.7491\n",
      "Epoch 166/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8157 - accuracy: 0.7513\n",
      "Epoch 167/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8086 - accuracy: 0.7441\n",
      "Epoch 168/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7984 - accuracy: 0.7534\n",
      "Epoch 169/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8180 - accuracy: 0.7534\n",
      "Epoch 170/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8166 - accuracy: 0.7505\n",
      "Epoch 171/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8060 - accuracy: 0.7513\n",
      "Epoch 172/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8375 - accuracy: 0.7505\n",
      "Epoch 173/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8462 - accuracy: 0.7470\n",
      "Epoch 174/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8100 - accuracy: 0.7520\n",
      "Epoch 175/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8056 - accuracy: 0.7548\n",
      "Epoch 176/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7930 - accuracy: 0.7556\n",
      "Epoch 177/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8094 - accuracy: 0.7491\n",
      "Epoch 178/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8136 - accuracy: 0.7491\n",
      "Epoch 179/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7975 - accuracy: 0.7556\n",
      "Epoch 180/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7860 - accuracy: 0.7584\n",
      "Epoch 181/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7878 - accuracy: 0.7577\n",
      "Epoch 182/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7966 - accuracy: 0.7498\n",
      "Epoch 183/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7836 - accuracy: 0.7577\n",
      "Epoch 184/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7912 - accuracy: 0.7520\n",
      "Epoch 185/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8089 - accuracy: 0.7491\n",
      "Epoch 186/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7970 - accuracy: 0.7527\n",
      "Epoch 187/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7898 - accuracy: 0.7548\n",
      "Epoch 188/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7838 - accuracy: 0.7527\n",
      "Epoch 189/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7790 - accuracy: 0.7556\n",
      "Epoch 190/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8210 - accuracy: 0.7384\n",
      "Epoch 191/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.8206 - accuracy: 0.7448\n",
      "Epoch 192/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7990 - accuracy: 0.7462\n",
      "Epoch 193/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7937 - accuracy: 0.7513\n",
      "Epoch 194/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7861 - accuracy: 0.7548\n",
      "Epoch 195/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7823 - accuracy: 0.7591\n",
      "Epoch 196/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7780 - accuracy: 0.7577\n",
      "Epoch 197/200\n",
      "28/28 [==============================] - 0s 8ms/step - loss: 0.7782 - accuracy: 0.7577\n",
      "Epoch 198/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7731 - accuracy: 0.7620\n",
      "Epoch 199/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7664 - accuracy: 0.7599\n",
      "Epoch 200/200\n",
      "28/28 [==============================] - 0s 7ms/step - loss: 0.7690 - accuracy: 0.7606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24d8b3e6100>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=vanilla_rnn, epochs=200, batch_size=50, \n",
    "                       verbose=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "12/12 [==============================] - 0s 2ms/step\n",
      "0.7729549248747913\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "print(accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 층 더 쌓기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_vanilla_rnn():\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(50, input_shape=(49,1), return_sequences=True)) \n",
    "    # 중간층이므로 True로 만들어 다음 층에 연결해야 한다.\n",
    "    model.add(SimpleRNN(50, return_sequences=False)) \n",
    "\n",
    "    # 출력 : 50개, 훈련데이터의 개수 :(49,1)\n",
    "    model.add(Dense(46, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = Adam(lr=0.001), \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 2.4055 - accuracy: 0.5613\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 1.3771 - accuracy: 0.7147\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.1930 - accuracy: 0.7147 0s - loss: 1.2416 - accuracy\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.1671 - accuracy: 0.7147\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.1593 - accuracy: 0.7147\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.1555 - accuracy: 0.7147 0s - loss: 1.1496 - accuracy: 0.\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.1474 - accuracy: 0.7147\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.1229 - accuracy: 0.7147\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.1097 - accuracy: 0.7147\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0957 - accuracy: 0.7147\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0929 - accuracy: 0.7147\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 1.0870 - accuracy: 0.7147\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 1.0865 - accuracy: 0.7147\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0781 - accuracy: 0.7154 0s - loss: 1.0792 - accuracy: 0.71\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 1.0809 - accuracy: 0.7147\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0695 - accuracy: 0.7111\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0667 - accuracy: 0.7147\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0636 - accuracy: 0.7183\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 1.0550 - accuracy: 0.7147\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0543 - accuracy: 0.7125\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0506 - accuracy: 0.7154\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0458 - accuracy: 0.7111\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0486 - accuracy: 0.7133\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0421 - accuracy: 0.7140\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 1.0406 - accuracy: 0.7176\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0277 - accuracy: 0.7176\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0299 - accuracy: 0.7125\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0244 - accuracy: 0.7140\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0201 - accuracy: 0.7118\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 1.0170 - accuracy: 0.7226\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 1.0212 - accuracy: 0.7140\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 1.0260 - accuracy: 0.7154\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0121 - accuracy: 0.7140\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 1.0029 - accuracy: 0.7211\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.9947 - accuracy: 0.7233\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.9704 - accuracy: 0.7254\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.9739 - accuracy: 0.7262\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.9609 - accuracy: 0.7312\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.9584 - accuracy: 0.7297\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.9445 - accuracy: 0.7176\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.9527 - accuracy: 0.7312\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.9207 - accuracy: 0.7326\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.9174 - accuracy: 0.7412\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.9196 - accuracy: 0.7348\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.9067 - accuracy: 0.7376\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8976 - accuracy: 0.7384\n",
      "Epoch 47/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8953 - accuracy: 0.7419\n",
      "Epoch 48/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8901 - accuracy: 0.7412\n",
      "Epoch 49/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8989 - accuracy: 0.7412\n",
      "Epoch 50/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8767 - accuracy: 0.7419\n",
      "Epoch 51/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8783 - accuracy: 0.7427\n",
      "Epoch 52/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8670 - accuracy: 0.7455\n",
      "Epoch 53/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8691 - accuracy: 0.7477\n",
      "Epoch 54/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.9083 - accuracy: 0.7362\n",
      "Epoch 55/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8742 - accuracy: 0.7391\n",
      "Epoch 56/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8638 - accuracy: 0.7376\n",
      "Epoch 57/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8710 - accuracy: 0.7419\n",
      "Epoch 58/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8532 - accuracy: 0.7455\n",
      "Epoch 59/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8559 - accuracy: 0.7470\n",
      "Epoch 60/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8507 - accuracy: 0.7434\n",
      "Epoch 61/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8390 - accuracy: 0.7498\n",
      "Epoch 62/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8462 - accuracy: 0.7434\n",
      "Epoch 63/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8659 - accuracy: 0.7448\n",
      "Epoch 64/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8409 - accuracy: 0.7484\n",
      "Epoch 65/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8316 - accuracy: 0.7534\n",
      "Epoch 66/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8414 - accuracy: 0.7513\n",
      "Epoch 67/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8234 - accuracy: 0.7484\n",
      "Epoch 68/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8188 - accuracy: 0.7556\n",
      "Epoch 69/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8229 - accuracy: 0.7513\n",
      "Epoch 70/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8013 - accuracy: 0.7591\n",
      "Epoch 71/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8026 - accuracy: 0.7620\n",
      "Epoch 72/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8066 - accuracy: 0.7556\n",
      "Epoch 73/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.9573 - accuracy: 0.7168\n",
      "Epoch 74/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 1.0373 - accuracy: 0.6817\n",
      "Epoch 75/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.9004 - accuracy: 0.7348\n",
      "Epoch 76/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8659 - accuracy: 0.7355\n",
      "Epoch 77/200\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 0.8567 - accuracy: 0.7341\n",
      "Epoch 78/200\n",
      "28/28 [==============================] - 0s 17ms/step - loss: 0.8538 - accuracy: 0.7398\n",
      "Epoch 79/200\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 0.8428 - accuracy: 0.7391\n",
      "Epoch 80/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8424 - accuracy: 0.7405\n",
      "Epoch 81/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8370 - accuracy: 0.7398\n",
      "Epoch 82/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8280 - accuracy: 0.7412\n",
      "Epoch 83/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8446 - accuracy: 0.7355\n",
      "Epoch 84/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8238 - accuracy: 0.7398\n",
      "Epoch 85/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8187 - accuracy: 0.7412\n",
      "Epoch 86/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8220 - accuracy: 0.7391\n",
      "Epoch 87/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8162 - accuracy: 0.7427\n",
      "Epoch 88/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8226 - accuracy: 0.7427\n",
      "Epoch 89/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8078 - accuracy: 0.7484\n",
      "Epoch 90/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8110 - accuracy: 0.7455\n",
      "Epoch 91/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8069 - accuracy: 0.7434\n",
      "Epoch 92/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8082 - accuracy: 0.7470\n",
      "Epoch 93/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.8110 - accuracy: 0.7384\n",
      "Epoch 94/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.8039 - accuracy: 0.7462\n",
      "Epoch 95/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7947 - accuracy: 0.7441\n",
      "Epoch 96/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7986 - accuracy: 0.7484\n",
      "Epoch 97/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7958 - accuracy: 0.7462\n",
      "Epoch 98/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7903 - accuracy: 0.7477\n",
      "Epoch 99/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7811 - accuracy: 0.7577\n",
      "Epoch 100/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7874 - accuracy: 0.7505\n",
      "Epoch 101/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7831 - accuracy: 0.7563\n",
      "Epoch 102/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7804 - accuracy: 0.7577\n",
      "Epoch 103/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7811 - accuracy: 0.7520\n",
      "Epoch 104/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7733 - accuracy: 0.7556\n",
      "Epoch 105/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7734 - accuracy: 0.7520\n",
      "Epoch 106/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7713 - accuracy: 0.7541\n",
      "Epoch 107/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7645 - accuracy: 0.7556\n",
      "Epoch 108/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.7657 - accuracy: 0.7541\n",
      "Epoch 109/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.7861 - accuracy: 0.7498\n",
      "Epoch 110/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7588 - accuracy: 0.7520\n",
      "Epoch 111/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7566 - accuracy: 0.7556\n",
      "Epoch 112/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7723 - accuracy: 0.7599\n",
      "Epoch 113/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.7496 - accuracy: 0.7570\n",
      "Epoch 114/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7458 - accuracy: 0.7606\n",
      "Epoch 115/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7464 - accuracy: 0.7577\n",
      "Epoch 116/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.7393 - accuracy: 0.7563\n",
      "Epoch 117/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7383 - accuracy: 0.7584\n",
      "Epoch 118/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7381 - accuracy: 0.7642\n",
      "Epoch 119/200\n",
      "28/28 [==============================] - 0s 16ms/step - loss: 0.7347 - accuracy: 0.7613\n",
      "Epoch 120/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.7294 - accuracy: 0.7613\n",
      "Epoch 121/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7365 - accuracy: 0.7599\n",
      "Epoch 122/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.7245 - accuracy: 0.7663\n",
      "Epoch 123/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7267 - accuracy: 0.7613\n",
      "Epoch 124/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7239 - accuracy: 0.7634\n",
      "Epoch 125/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7219 - accuracy: 0.7620\n",
      "Epoch 126/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7084 - accuracy: 0.7642\n",
      "Epoch 127/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7074 - accuracy: 0.7642\n",
      "Epoch 128/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7048 - accuracy: 0.7713\n",
      "Epoch 129/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7093 - accuracy: 0.7677\n",
      "Epoch 130/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7089 - accuracy: 0.7677\n",
      "Epoch 131/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6941 - accuracy: 0.7670\n",
      "Epoch 132/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7051 - accuracy: 0.7627\n",
      "Epoch 133/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6989 - accuracy: 0.7642\n",
      "Epoch 134/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.6899 - accuracy: 0.7685\n",
      "Epoch 135/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6819 - accuracy: 0.7677\n",
      "Epoch 136/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6871 - accuracy: 0.7642\n",
      "Epoch 137/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.6860 - accuracy: 0.7728\n",
      "Epoch 138/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6821 - accuracy: 0.7699\n",
      "Epoch 139/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6948 - accuracy: 0.7584\n",
      "Epoch 140/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6718 - accuracy: 0.7728\n",
      "Epoch 141/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.6755 - accuracy: 0.7735\n",
      "Epoch 142/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6717 - accuracy: 0.7771\n",
      "Epoch 143/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.6551 - accuracy: 0.7799\n",
      "Epoch 144/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6571 - accuracy: 0.7806\n",
      "Epoch 145/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6686 - accuracy: 0.7735\n",
      "Epoch 146/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6573 - accuracy: 0.7735\n",
      "Epoch 147/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.6540 - accuracy: 0.7799\n",
      "Epoch 148/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6452 - accuracy: 0.7771\n",
      "Epoch 149/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.6877 - accuracy: 0.7627\n",
      "Epoch 150/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6689 - accuracy: 0.7677\n",
      "Epoch 151/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6490 - accuracy: 0.7806 0s - loss: 0.6396 - accuracy: 0.\n",
      "Epoch 152/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6511 - accuracy: 0.7792\n",
      "Epoch 153/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6379 - accuracy: 0.7785\n",
      "Epoch 154/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.6347 - accuracy: 0.7821\n",
      "Epoch 155/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6281 - accuracy: 0.7871\n",
      "Epoch 156/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6401 - accuracy: 0.7677\n",
      "Epoch 157/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6432 - accuracy: 0.7806 0s - loss: 0.6556 - accu\n",
      "Epoch 158/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.6416 - accuracy: 0.7806\n",
      "Epoch 159/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6567 - accuracy: 0.7763\n",
      "Epoch 160/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6357 - accuracy: 0.7857\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6344 - accuracy: 0.7749\n",
      "Epoch 162/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.6251 - accuracy: 0.7864\n",
      "Epoch 163/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6266 - accuracy: 0.7871\n",
      "Epoch 164/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6128 - accuracy: 0.7806\n",
      "Epoch 165/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6049 - accuracy: 0.7878\n",
      "Epoch 166/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6164 - accuracy: 0.7842\n",
      "Epoch 167/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6111 - accuracy: 0.7842\n",
      "Epoch 168/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5896 - accuracy: 0.7986\n",
      "Epoch 169/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5866 - accuracy: 0.7993\n",
      "Epoch 170/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.6693 - accuracy: 0.7634\n",
      "Epoch 171/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6419 - accuracy: 0.7749\n",
      "Epoch 172/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.6311 - accuracy: 0.7778\n",
      "Epoch 173/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5939 - accuracy: 0.7957\n",
      "Epoch 174/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5699 - accuracy: 0.8000\n",
      "Epoch 175/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.5904 - accuracy: 0.8007\n",
      "Epoch 176/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6554 - accuracy: 0.7677\n",
      "Epoch 177/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6586 - accuracy: 0.7692\n",
      "Epoch 178/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.6159 - accuracy: 0.7749\n",
      "Epoch 179/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5772 - accuracy: 0.7950\n",
      "Epoch 180/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.6405 - accuracy: 0.7806\n",
      "Epoch 181/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.7213 - accuracy: 0.7427\n",
      "Epoch 182/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6711 - accuracy: 0.7620 0s - loss: 0.6026 - accura\n",
      "Epoch 183/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6431 - accuracy: 0.7685\n",
      "Epoch 184/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6398 - accuracy: 0.7821\n",
      "Epoch 185/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6311 - accuracy: 0.7763\n",
      "Epoch 186/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6151 - accuracy: 0.7792\n",
      "Epoch 187/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.6133 - accuracy: 0.7828\n",
      "Epoch 188/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5945 - accuracy: 0.7892\n",
      "Epoch 189/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5905 - accuracy: 0.7921\n",
      "Epoch 190/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5876 - accuracy: 0.7921\n",
      "Epoch 191/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.5754 - accuracy: 0.7971\n",
      "Epoch 192/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5730 - accuracy: 0.7914\n",
      "Epoch 193/200\n",
      "28/28 [==============================] - 0s 15ms/step - loss: 0.5643 - accuracy: 0.8050\n",
      "Epoch 194/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5658 - accuracy: 0.7957\n",
      "Epoch 195/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5503 - accuracy: 0.8014\n",
      "Epoch 196/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5357 - accuracy: 0.8143 0s - loss: 0.5301 - accu\n",
      "Epoch 197/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5539 - accuracy: 0.8072\n",
      "Epoch 198/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5614 - accuracy: 0.8036\n",
      "Epoch 199/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5462 - accuracy: 0.8100\n",
      "Epoch 200/200\n",
      "28/28 [==============================] - 0s 14ms/step - loss: 0.5206 - accuracy: 0.8136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24d87e45460>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=stacked_vanilla_rnn, epochs=200, batch_size=50, \n",
    "                       verbose=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-18dcd7d3b280>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_test_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m     \"\"\"\n\u001b[1;32m-> 1186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_test_ = np.argmax(y_test,axis=1)\n",
    "print(accuracy_score(y_pred, y_test_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_rnn():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(49,1), return_sequences=False)) \n",
    "    # 출력 : 50개, 훈련데이터의 개수 :(49,1)\n",
    "    model.add(Dense(46, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=Adam(lr=0.001), \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 3.1626 - accuracy: 0.6079\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.3776 - accuracy: 0.7147\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1747 - accuracy: 0.7147\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.1318 - accuracy: 0.7147\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1140 - accuracy: 0.7147\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.1005 - accuracy: 0.7147\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 1.0963 - accuracy: 0.7147\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0973 - accuracy: 0.7147\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0963 - accuracy: 0.7147\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.0829 - accuracy: 0.7147\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.0747 - accuracy: 0.7147\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0625 - accuracy: 0.7147\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.1181 - accuracy: 0.7147\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.0994 - accuracy: 0.7147\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 1.0868 - accuracy: 0.7147\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 1.0467 - accuracy: 0.7147\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.9885 - accuracy: 0.7147\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.9344 - accuracy: 0.7821\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.8689 - accuracy: 0.8029\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.8525 - accuracy: 0.8036\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.8396 - accuracy: 0.8029\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.8991 - accuracy: 0.7828\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.8630 - accuracy: 0.7878\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.8321 - accuracy: 0.7986\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.8457 - accuracy: 0.8022\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.8261 - accuracy: 0.8014\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.8277 - accuracy: 0.8050\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.8216 - accuracy: 0.8036\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.8370 - accuracy: 0.7914\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.8175 - accuracy: 0.7957\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.8308 - accuracy: 0.7799\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.8082 - accuracy: 0.7885\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.8085 - accuracy: 0.7864\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.8070 - accuracy: 0.7849\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.8213 - accuracy: 0.7935\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7937 - accuracy: 0.7957\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7837 - accuracy: 0.7993\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7938 - accuracy: 0.7950 0s - loss: 0.8003 - accuracy: 0.\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7798 - accuracy: 0.7935\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7770 - accuracy: 0.8022\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.7747 - accuracy: 0.8022\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7666 - accuracy: 0.7978\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7945 - accuracy: 0.7799\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7729 - accuracy: 0.8014\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7604 - accuracy: 0.8000\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.7547 - accuracy: 0.8022\n",
      "Epoch 47/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.7542 - accuracy: 0.8000\n",
      "Epoch 48/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.7559 - accuracy: 0.8036\n",
      "Epoch 49/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7494 - accuracy: 0.8050\n",
      "Epoch 50/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7456 - accuracy: 0.8172\n",
      "Epoch 51/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7501 - accuracy: 0.8093\n",
      "Epoch 52/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7282 - accuracy: 0.8143\n",
      "Epoch 53/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7136 - accuracy: 0.8294\n",
      "Epoch 54/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7187 - accuracy: 0.8244\n",
      "Epoch 55/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7313 - accuracy: 0.8186\n",
      "Epoch 56/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7189 - accuracy: 0.8272\n",
      "Epoch 57/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7278 - accuracy: 0.8222\n",
      "Epoch 58/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.7106 - accuracy: 0.8244\n",
      "Epoch 59/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6930 - accuracy: 0.8294\n",
      "Epoch 60/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6813 - accuracy: 0.8330\n",
      "Epoch 61/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6822 - accuracy: 0.8323\n",
      "Epoch 62/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6743 - accuracy: 0.8337\n",
      "Epoch 63/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6745 - accuracy: 0.8351\n",
      "Epoch 64/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.6777 - accuracy: 0.8323\n",
      "Epoch 65/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.7014 - accuracy: 0.8287\n",
      "Epoch 66/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6689 - accuracy: 0.8330\n",
      "Epoch 67/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6766 - accuracy: 0.8308\n",
      "Epoch 68/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6736 - accuracy: 0.8323\n",
      "Epoch 69/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6639 - accuracy: 0.8358\n",
      "Epoch 70/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.6557 - accuracy: 0.8344\n",
      "Epoch 71/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6588 - accuracy: 0.8373\n",
      "Epoch 72/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6582 - accuracy: 0.8323\n",
      "Epoch 73/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6585 - accuracy: 0.8373\n",
      "Epoch 74/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6486 - accuracy: 0.8380\n",
      "Epoch 75/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6528 - accuracy: 0.8351\n",
      "Epoch 76/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.6477 - accuracy: 0.8358\n",
      "Epoch 77/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6552 - accuracy: 0.8380\n",
      "Epoch 78/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6600 - accuracy: 0.8358\n",
      "Epoch 79/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6456 - accuracy: 0.8351\n",
      "Epoch 80/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6440 - accuracy: 0.8358\n",
      "Epoch 81/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.6506 - accuracy: 0.8344\n",
      "Epoch 82/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6419 - accuracy: 0.8358\n",
      "Epoch 83/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6293 - accuracy: 0.8373\n",
      "Epoch 84/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6540 - accuracy: 0.8308\n",
      "Epoch 85/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6360 - accuracy: 0.8401\n",
      "Epoch 86/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6232 - accuracy: 0.8394\n",
      "Epoch 87/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.6333 - accuracy: 0.8416\n",
      "Epoch 88/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6369 - accuracy: 0.8394 0s - loss: 0.6534 - accuracy: 0.\n",
      "Epoch 89/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6468 - accuracy: 0.8358\n",
      "Epoch 90/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6109 - accuracy: 0.8452\n",
      "Epoch 91/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6048 - accuracy: 0.8459\n",
      "Epoch 92/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6145 - accuracy: 0.8423\n",
      "Epoch 93/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.6164 - accuracy: 0.8373\n",
      "Epoch 94/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6323 - accuracy: 0.8358\n",
      "Epoch 95/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6146 - accuracy: 0.8409\n",
      "Epoch 96/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6257 - accuracy: 0.8380\n",
      "Epoch 97/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5975 - accuracy: 0.8487\n",
      "Epoch 98/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6098 - accuracy: 0.8437\n",
      "Epoch 99/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.6010 - accuracy: 0.8430 0s - loss: 0.6485 - accuracy\n",
      "Epoch 100/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5939 - accuracy: 0.8444\n",
      "Epoch 101/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6108 - accuracy: 0.8430\n",
      "Epoch 102/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5972 - accuracy: 0.8409\n",
      "Epoch 103/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5849 - accuracy: 0.8473\n",
      "Epoch 104/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5869 - accuracy: 0.8473\n",
      "Epoch 105/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5762 - accuracy: 0.8452\n",
      "Epoch 106/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5665 - accuracy: 0.8480\n",
      "Epoch 107/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6036 - accuracy: 0.8452\n",
      "Epoch 108/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5975 - accuracy: 0.8452\n",
      "Epoch 109/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5829 - accuracy: 0.8495\n",
      "Epoch 110/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.7376 - accuracy: 0.8093\n",
      "Epoch 111/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6404 - accuracy: 0.8416\n",
      "Epoch 112/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6194 - accuracy: 0.8423\n",
      "Epoch 113/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.6100 - accuracy: 0.8409\n",
      "Epoch 114/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5844 - accuracy: 0.8437\n",
      "Epoch 115/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.5683 - accuracy: 0.8466\n",
      "Epoch 116/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5720 - accuracy: 0.8473\n",
      "Epoch 117/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5523 - accuracy: 0.8487\n",
      "Epoch 118/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5587 - accuracy: 0.8466\n",
      "Epoch 119/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.5434 - accuracy: 0.8480\n",
      "Epoch 120/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5457 - accuracy: 0.8516\n",
      "Epoch 121/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5387 - accuracy: 0.8581\n",
      "Epoch 122/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.5386 - accuracy: 0.8523\n",
      "Epoch 123/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5369 - accuracy: 0.8538\n",
      "Epoch 124/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5228 - accuracy: 0.8595\n",
      "Epoch 125/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.5263 - accuracy: 0.8552\n",
      "Epoch 126/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.5316 - accuracy: 0.8595\n",
      "Epoch 127/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5158 - accuracy: 0.8595\n",
      "Epoch 128/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5329 - accuracy: 0.8552\n",
      "Epoch 129/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5203 - accuracy: 0.8530\n",
      "Epoch 130/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5034 - accuracy: 0.8609\n",
      "Epoch 131/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5010 - accuracy: 0.8674\n",
      "Epoch 132/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5222 - accuracy: 0.8602\n",
      "Epoch 133/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5091 - accuracy: 0.8631\n",
      "Epoch 134/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5060 - accuracy: 0.8674 0s - loss: 0.4841 - accuracy\n",
      "Epoch 135/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5224 - accuracy: 0.8638\n",
      "Epoch 136/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5091 - accuracy: 0.8645\n",
      "Epoch 137/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4963 - accuracy: 0.8659\n",
      "Epoch 138/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5046 - accuracy: 0.8645\n",
      "Epoch 139/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4937 - accuracy: 0.8724\n",
      "Epoch 140/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4840 - accuracy: 0.8703\n",
      "Epoch 141/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4830 - accuracy: 0.8738\n",
      "Epoch 142/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4953 - accuracy: 0.8703\n",
      "Epoch 143/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.5135 - accuracy: 0.8638\n",
      "Epoch 144/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4919 - accuracy: 0.8681\n",
      "Epoch 145/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4908 - accuracy: 0.8667\n",
      "Epoch 146/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4772 - accuracy: 0.8703\n",
      "Epoch 147/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4688 - accuracy: 0.8731\n",
      "Epoch 148/200\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.5057 - accuracy: 0.8631\n",
      "Epoch 149/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.5454 - accuracy: 0.8602\n",
      "Epoch 150/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4829 - accuracy: 0.8695\n",
      "Epoch 151/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4765 - accuracy: 0.8703\n",
      "Epoch 152/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4512 - accuracy: 0.8810\n",
      "Epoch 153/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4486 - accuracy: 0.8817\n",
      "Epoch 154/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4404 - accuracy: 0.8817\n",
      "Epoch 155/200\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.4352 - accuracy: 0.8824\n",
      "Epoch 156/200\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.4368 - accuracy: 0.8789\n",
      "Epoch 157/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4427 - accuracy: 0.8839\n",
      "Epoch 158/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4522 - accuracy: 0.8810\n",
      "Epoch 159/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4416 - accuracy: 0.8817\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4678 - accuracy: 0.8731\n",
      "Epoch 161/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4408 - accuracy: 0.8796\n",
      "Epoch 162/200\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.4358 - accuracy: 0.8824\n",
      "Epoch 163/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4562 - accuracy: 0.8753\n",
      "Epoch 164/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4418 - accuracy: 0.8803\n",
      "Epoch 165/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4344 - accuracy: 0.8803 0s - loss: 0.4296 - accuracy\n",
      "Epoch 166/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4241 - accuracy: 0.8846\n",
      "Epoch 167/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4313 - accuracy: 0.8810 0s - loss: 0.4124 - accuracy\n",
      "Epoch 168/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4508 - accuracy: 0.8774\n",
      "Epoch 169/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4231 - accuracy: 0.8810\n",
      "Epoch 170/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4215 - accuracy: 0.8817\n",
      "Epoch 171/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4107 - accuracy: 0.8910\n",
      "Epoch 172/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.4331 - accuracy: 0.8803\n",
      "Epoch 173/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4153 - accuracy: 0.8860\n",
      "Epoch 174/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4216 - accuracy: 0.8889\n",
      "Epoch 175/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4153 - accuracy: 0.8824\n",
      "Epoch 176/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4148 - accuracy: 0.8853\n",
      "Epoch 177/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4028 - accuracy: 0.8910\n",
      "Epoch 178/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.3963 - accuracy: 0.8853\n",
      "Epoch 179/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4051 - accuracy: 0.8896\n",
      "Epoch 180/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4100 - accuracy: 0.8860\n",
      "Epoch 181/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.4128 - accuracy: 0.8853\n",
      "Epoch 182/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.3965 - accuracy: 0.8860\n",
      "Epoch 183/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.3949 - accuracy: 0.8896\n",
      "Epoch 184/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.3892 - accuracy: 0.8896\n",
      "Epoch 185/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.3795 - accuracy: 0.8925\n",
      "Epoch 186/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.3876 - accuracy: 0.8860\n",
      "Epoch 187/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.3835 - accuracy: 0.8968\n",
      "Epoch 188/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.3683 - accuracy: 0.8961\n",
      "Epoch 189/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.3676 - accuracy: 0.8953\n",
      "Epoch 190/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.3601 - accuracy: 0.8961 0s - loss: 0.3683 - accuracy: 0.\n",
      "Epoch 191/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.3594 - accuracy: 0.8968\n",
      "Epoch 192/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.3670 - accuracy: 0.8975\n",
      "Epoch 193/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.3744 - accuracy: 0.8932\n",
      "Epoch 194/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.3538 - accuracy: 0.9004\n",
      "Epoch 195/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.3606 - accuracy: 0.8932\n",
      "Epoch 196/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.3578 - accuracy: 0.8982\n",
      "Epoch 197/200\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.3488 - accuracy: 0.9018\n",
      "Epoch 198/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.3413 - accuracy: 0.9039\n",
      "Epoch 199/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.3494 - accuracy: 0.9032\n",
      "Epoch 200/200\n",
      "28/28 [==============================] - 1s 19ms/step - loss: 0.3422 - accuracy: 0.9047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24d8b55e280>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=lstm_rnn, epochs=200, batch_size=50, \n",
    "                       verbose=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 5ms/step\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-7b405a81e17f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my_test_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m     \"\"\"\n\u001b[1;32m-> 1186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = model.predict(X_test)\n",
    "y_test_ = np.argmax(y_test,axis=1)\n",
    "print(accuracy_score(y_pred, y_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다층(multy layer, staked) LSTM\n",
    "\n",
    "def staked_lstm_rnn():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(49,1), return_sequences=True)) \n",
    "    # 출력 : 50개, 훈련데이터의 개수 :(49,1)\n",
    "    model.add(LSTM(50, return_sequences=False)) \n",
    "    \n",
    "    model.add(Dense(46, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=Adam(lr=0.001), \n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 2.5775 - accuracy: 0.6509\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 1.2966 - accuracy: 0.7147\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 1.1874 - accuracy: 0.7147\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 1.1711 - accuracy: 0.7147\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 1.1419 - accuracy: 0.7147\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 1.1182 - accuracy: 0.7147\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 1.0984 - accuracy: 0.7147\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 1.0167 - accuracy: 0.7147\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.8960 - accuracy: 0.7785\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.8578 - accuracy: 0.8014\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 1s 49ms/step - loss: 0.8671 - accuracy: 0.7864\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 2s 65ms/step - loss: 0.8280 - accuracy: 0.8036\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.8153 - accuracy: 0.7935\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 1s 43ms/step - loss: 0.7999 - accuracy: 0.7971\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.7974 - accuracy: 0.8065\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.7857 - accuracy: 0.8036\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.7946 - accuracy: 0.7993\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.7888 - accuracy: 0.8029\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.8182 - accuracy: 0.7957\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 1s 43ms/step - loss: 0.7910 - accuracy: 0.7957\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.8069 - accuracy: 0.7986\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.7897 - accuracy: 0.8014\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.7623 - accuracy: 0.8151\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.7513 - accuracy: 0.8115\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 1s 43ms/step - loss: 0.7473 - accuracy: 0.8186\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.7612 - accuracy: 0.8143\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.7536 - accuracy: 0.8115\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.7421 - accuracy: 0.8158\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.7192 - accuracy: 0.8222\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.7120 - accuracy: 0.8251\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.7324 - accuracy: 0.8229\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.7555 - accuracy: 0.8179\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.7063 - accuracy: 0.8272\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.7275 - accuracy: 0.8194\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.7022 - accuracy: 0.8244\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.7055 - accuracy: 0.8222\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.7111 - accuracy: 0.8186 0s - loss: 0.6776 - ac\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 1s 43ms/step - loss: 0.6966 - accuracy: 0.8251\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6841 - accuracy: 0.8272\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6839 - accuracy: 0.8265\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 1s 43ms/step - loss: 0.6939 - accuracy: 0.8272\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.6844 - accuracy: 0.8308\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6649 - accuracy: 0.8380\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6819 - accuracy: 0.8337\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.6807 - accuracy: 0.8394\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6725 - accuracy: 0.8265\n",
      "Epoch 47/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6708 - accuracy: 0.8358\n",
      "Epoch 48/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.6653 - accuracy: 0.8344\n",
      "Epoch 49/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6630 - accuracy: 0.8380 0s - loss: 0.6717 - accuracy: 0.\n",
      "Epoch 50/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.6633 - accuracy: 0.8344\n",
      "Epoch 51/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.6721 - accuracy: 0.8323\n",
      "Epoch 52/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.6524 - accuracy: 0.8366\n",
      "Epoch 53/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6504 - accuracy: 0.8380\n",
      "Epoch 54/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6475 - accuracy: 0.8366\n",
      "Epoch 55/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6443 - accuracy: 0.8366\n",
      "Epoch 56/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6325 - accuracy: 0.8444\n",
      "Epoch 57/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.6304 - accuracy: 0.8437\n",
      "Epoch 58/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6354 - accuracy: 0.8444\n",
      "Epoch 59/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6763 - accuracy: 0.8373\n",
      "Epoch 60/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.6269 - accuracy: 0.8444\n",
      "Epoch 61/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6212 - accuracy: 0.8452\n",
      "Epoch 62/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6097 - accuracy: 0.8452\n",
      "Epoch 63/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6072 - accuracy: 0.8480\n",
      "Epoch 64/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6084 - accuracy: 0.8459\n",
      "Epoch 65/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6082 - accuracy: 0.8473\n",
      "Epoch 66/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6068 - accuracy: 0.8487\n",
      "Epoch 67/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.6179 - accuracy: 0.8401\n",
      "Epoch 68/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.5959 - accuracy: 0.8480\n",
      "Epoch 69/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.5897 - accuracy: 0.8487\n",
      "Epoch 70/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.5943 - accuracy: 0.8473\n",
      "Epoch 71/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.5858 - accuracy: 0.8502\n",
      "Epoch 72/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.6138 - accuracy: 0.8366\n",
      "Epoch 73/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.5729 - accuracy: 0.8437\n",
      "Epoch 74/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.5565 - accuracy: 0.8581\n",
      "Epoch 75/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.5629 - accuracy: 0.8581\n",
      "Epoch 76/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.5936 - accuracy: 0.8473\n",
      "Epoch 77/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.5660 - accuracy: 0.8509\n",
      "Epoch 78/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.5553 - accuracy: 0.8602\n",
      "Epoch 79/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.5448 - accuracy: 0.8624\n",
      "Epoch 80/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.5487 - accuracy: 0.8638\n",
      "Epoch 81/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.5413 - accuracy: 0.8609\n",
      "Epoch 82/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.5378 - accuracy: 0.8624\n",
      "Epoch 83/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.5662 - accuracy: 0.8573\n",
      "Epoch 84/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.5629 - accuracy: 0.8566\n",
      "Epoch 85/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.5238 - accuracy: 0.8652\n",
      "Epoch 86/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.5282 - accuracy: 0.8624\n",
      "Epoch 87/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.5376 - accuracy: 0.8609\n",
      "Epoch 88/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.5060 - accuracy: 0.8695\n",
      "Epoch 89/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.5566 - accuracy: 0.8566\n",
      "Epoch 90/200\n",
      "28/28 [==============================] - 1s 43ms/step - loss: 0.5212 - accuracy: 0.8631\n",
      "Epoch 91/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.4972 - accuracy: 0.8724\n",
      "Epoch 92/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.4839 - accuracy: 0.8746\n",
      "Epoch 93/200\n",
      "28/28 [==============================] - 1s 43ms/step - loss: 0.5081 - accuracy: 0.8659\n",
      "Epoch 94/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.4643 - accuracy: 0.8860\n",
      "Epoch 95/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.4620 - accuracy: 0.8810\n",
      "Epoch 96/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.4531 - accuracy: 0.8860\n",
      "Epoch 97/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.5078 - accuracy: 0.8688 0s - loss:\n",
      "Epoch 98/200\n",
      "28/28 [==============================] - 1s 43ms/step - loss: 0.4909 - accuracy: 0.8753\n",
      "Epoch 99/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.4393 - accuracy: 0.8889\n",
      "Epoch 100/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.4421 - accuracy: 0.8817\n",
      "Epoch 101/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.4310 - accuracy: 0.8882\n",
      "Epoch 102/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.4139 - accuracy: 0.8910\n",
      "Epoch 103/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.4123 - accuracy: 0.8882\n",
      "Epoch 104/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.4069 - accuracy: 0.8910 0s - loss: 0.4173 - accura\n",
      "Epoch 105/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.4174 - accuracy: 0.8882\n",
      "Epoch 106/200\n",
      "28/28 [==============================] - 1s 43ms/step - loss: 0.4082 - accuracy: 0.8925\n",
      "Epoch 107/200\n",
      "28/28 [==============================] - 1s 47ms/step - loss: 0.4042 - accuracy: 0.8989\n",
      "Epoch 108/200\n",
      "28/28 [==============================] - 1s 45ms/step - loss: 0.3913 - accuracy: 0.8953\n",
      "Epoch 109/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.4342 - accuracy: 0.8746\n",
      "Epoch 110/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.4004 - accuracy: 0.8925\n",
      "Epoch 111/200\n",
      "28/28 [==============================] - 1s 43ms/step - loss: 0.3819 - accuracy: 0.8946\n",
      "Epoch 112/200\n",
      "28/28 [==============================] - 1s 45ms/step - loss: 0.3737 - accuracy: 0.9032\n",
      "Epoch 113/200\n",
      "28/28 [==============================] - 1s 50ms/step - loss: 0.4143 - accuracy: 0.8889\n",
      "Epoch 114/200\n",
      "28/28 [==============================] - 1s 44ms/step - loss: 0.4473 - accuracy: 0.8774\n",
      "Epoch 115/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.3823 - accuracy: 0.8996\n",
      "Epoch 116/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.3622 - accuracy: 0.9061\n",
      "Epoch 117/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.3477 - accuracy: 0.9082\n",
      "Epoch 118/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.3829 - accuracy: 0.8996\n",
      "Epoch 119/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.3654 - accuracy: 0.9075\n",
      "Epoch 120/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.3503 - accuracy: 0.9090\n",
      "Epoch 121/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.3448 - accuracy: 0.9118\n",
      "Epoch 122/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.3368 - accuracy: 0.9111\n",
      "Epoch 123/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.3297 - accuracy: 0.9111\n",
      "Epoch 124/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.3164 - accuracy: 0.9154 0s - loss: 0.3290 - accura\n",
      "Epoch 125/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.3134 - accuracy: 0.9219\n",
      "Epoch 126/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.3231 - accuracy: 0.9204\n",
      "Epoch 127/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.3335 - accuracy: 0.9140\n",
      "Epoch 128/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.3157 - accuracy: 0.9168\n",
      "Epoch 129/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.3224 - accuracy: 0.9190\n",
      "Epoch 130/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.2977 - accuracy: 0.9240\n",
      "Epoch 131/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.3147 - accuracy: 0.9183\n",
      "Epoch 132/200\n",
      "28/28 [==============================] - 1s 43ms/step - loss: 0.3016 - accuracy: 0.9197\n",
      "Epoch 133/200\n",
      "28/28 [==============================] - 1s 48ms/step - loss: 0.2984 - accuracy: 0.9247\n",
      "Epoch 134/200\n",
      "28/28 [==============================] - 1s 49ms/step - loss: 0.3479 - accuracy: 0.9082\n",
      "Epoch 135/200\n",
      "28/28 [==============================] - 1s 44ms/step - loss: 0.3573 - accuracy: 0.8975\n",
      "Epoch 136/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.2913 - accuracy: 0.9240\n",
      "Epoch 137/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.2818 - accuracy: 0.9290\n",
      "Epoch 138/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.2671 - accuracy: 0.9355\n",
      "Epoch 139/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.2664 - accuracy: 0.9355\n",
      "Epoch 140/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.2701 - accuracy: 0.9297\n",
      "Epoch 141/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.2608 - accuracy: 0.9333\n",
      "Epoch 142/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.2504 - accuracy: 0.9348\n",
      "Epoch 143/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.2462 - accuracy: 0.9369\n",
      "Epoch 144/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.2350 - accuracy: 0.9434\n",
      "Epoch 145/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.2292 - accuracy: 0.9462\n",
      "Epoch 146/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.2256 - accuracy: 0.9448\n",
      "Epoch 147/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.2411 - accuracy: 0.9362\n",
      "Epoch 148/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.2460 - accuracy: 0.9348\n",
      "Epoch 149/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.2379 - accuracy: 0.9362\n",
      "Epoch 150/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.2417 - accuracy: 0.9391\n",
      "Epoch 151/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.2279 - accuracy: 0.9455\n",
      "Epoch 152/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.2208 - accuracy: 0.9441 0s - los\n",
      "Epoch 153/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.3321 - accuracy: 0.9111\n",
      "Epoch 154/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.3020 - accuracy: 0.9118\n",
      "Epoch 155/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.2385 - accuracy: 0.9419\n",
      "Epoch 156/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.2247 - accuracy: 0.9427\n",
      "Epoch 157/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.2136 - accuracy: 0.9491\n",
      "Epoch 158/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1993 - accuracy: 0.9484\n",
      "Epoch 159/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1898 - accuracy: 0.9577\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1841 - accuracy: 0.9599\n",
      "Epoch 161/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1936 - accuracy: 0.9534\n",
      "Epoch 162/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.2212 - accuracy: 0.9412\n",
      "Epoch 163/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1918 - accuracy: 0.9548\n",
      "Epoch 164/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.1916 - accuracy: 0.9541\n",
      "Epoch 165/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.2018 - accuracy: 0.9513\n",
      "Epoch 166/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1950 - accuracy: 0.9520\n",
      "Epoch 167/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1881 - accuracy: 0.9520\n",
      "Epoch 168/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.2126 - accuracy: 0.9427\n",
      "Epoch 169/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1916 - accuracy: 0.9534 0s - loss:\n",
      "Epoch 170/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1798 - accuracy: 0.9563\n",
      "Epoch 171/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.1731 - accuracy: 0.9563\n",
      "Epoch 172/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.1635 - accuracy: 0.9620\n",
      "Epoch 173/200\n",
      "28/28 [==============================] - 1s 37ms/step - loss: 0.1683 - accuracy: 0.9584\n",
      "Epoch 174/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.1575 - accuracy: 0.9613\n",
      "Epoch 175/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.1540 - accuracy: 0.9656\n",
      "Epoch 176/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.1587 - accuracy: 0.9613\n",
      "Epoch 177/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.2071 - accuracy: 0.9434\n",
      "Epoch 178/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.2145 - accuracy: 0.9434\n",
      "Epoch 179/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.1855 - accuracy: 0.9527\n",
      "Epoch 180/200\n",
      "28/28 [==============================] - 1s 37ms/step - loss: 0.1700 - accuracy: 0.9599\n",
      "Epoch 181/200\n",
      "28/28 [==============================] - 1s 38ms/step - loss: 0.1667 - accuracy: 0.9584\n",
      "Epoch 182/200\n",
      "28/28 [==============================] - 1s 37ms/step - loss: 0.1633 - accuracy: 0.9591\n",
      "Epoch 183/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1591 - accuracy: 0.9642\n",
      "Epoch 184/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.1656 - accuracy: 0.9649\n",
      "Epoch 185/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.1482 - accuracy: 0.9670 0s - loss: 0.1337 - ac\n",
      "Epoch 186/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.1462 - accuracy: 0.9677\n",
      "Epoch 187/200\n",
      "28/28 [==============================] - 1s 50ms/step - loss: 0.1420 - accuracy: 0.9692\n",
      "Epoch 188/200\n",
      "28/28 [==============================] - 1s 49ms/step - loss: 0.1309 - accuracy: 0.9706\n",
      "Epoch 189/200\n",
      "28/28 [==============================] - 1s 46ms/step - loss: 0.1267 - accuracy: 0.9713\n",
      "Epoch 190/200\n",
      "28/28 [==============================] - 1s 42ms/step - loss: 0.1282 - accuracy: 0.9728\n",
      "Epoch 191/200\n",
      "28/28 [==============================] - 1s 41ms/step - loss: 0.1252 - accuracy: 0.9699\n",
      "Epoch 192/200\n",
      "28/28 [==============================] - 1s 50ms/step - loss: 0.1196 - accuracy: 0.9735\n",
      "Epoch 193/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1277 - accuracy: 0.9699\n",
      "Epoch 194/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1426 - accuracy: 0.9699\n",
      "Epoch 195/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1723 - accuracy: 0.9599\n",
      "Epoch 196/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1444 - accuracy: 0.9656\n",
      "Epoch 197/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.1247 - accuracy: 0.9699\n",
      "Epoch 198/200\n",
      "28/28 [==============================] - 1s 43ms/step - loss: 0.1308 - accuracy: 0.9663\n",
      "Epoch 199/200\n",
      "28/28 [==============================] - 1s 40ms/step - loss: 0.1325 - accuracy: 0.9692\n",
      "Epoch 200/200\n",
      "28/28 [==============================] - 1s 39ms/step - loss: 0.1552 - accuracy: 0.9627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24d936e26a0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=staked_lstm_rnn, epochs=200, batch_size=50, \n",
    "                       verbose=1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-f977e6d16d26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_test_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m     \"\"\"\n\u001b[1;32m-> 1186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_test_1 = np.argmax(y_test,axis=1)\n",
    "print(accuracy_score(y_pred, y_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(599,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
